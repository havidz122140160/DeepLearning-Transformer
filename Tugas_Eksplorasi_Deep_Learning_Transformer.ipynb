{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Eksplorasi Arsitektur Transformer: Mesin Translasi English-French\n",
        "\n",
        "**Kelompok:**\n",
        "* Havidz Ridho Pratama - 122140160\n",
        "* Royfran Roger Valentino - 122140239\n",
        "\n",
        "Proyek ini adalah implementasi arsitektur Transformer \"from scratch\" menggunakan PyTorch untuk tugas penerjemahan mesin. Fokus dari eksplorasi ini adalah pada kejelasan proses pembangunan model, mulai dari persiapan data hingga inferensi."
      ],
      "metadata": {
        "id": "FBLb0Ug31SDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Persiapan Data (Text Preprocessing)\n",
        "\n",
        "Tahap ini mencakup semua langkah yang diperlukan untuk mengubah data teks mentah (kalimat) menjadi format Tensor yang siap diproses oleh model.\n",
        "\n",
        "**Proses yang Dilakukan:**\n",
        "1.  **Memuat Data**: Dua file `.csv` (`small_vocab_en.csv` dan `small_vocab_fr.csv`) dibaca sebagai file teks biasa baris per baris, lalu digabungkan ke dalam satu DataFrame Pandas.\n",
        "2.  **Tokenisasi**: Kami menggunakan `spaCy` untuk tokenisasi. Model `en_core_web_sm` digunakan untuk bahasa Inggris dan `fr_core_news_sm` untuk bahasa Prancis.\n",
        "3.  **Membangun Vocabulary**: Sebuah `class Vocabulary` kustom dibuat untuk memetakan setiap kata unik ke sebuah indeks integer. Kami menyertakan token khusus (`<PAD>`, `<SOS>`, `<EOS>`, `<UNK>`) dan menetapkan `freq_threshold=2` untuk menyaring kata yang terlalu jarang muncul.\n",
        "4.  **Dataset Kustom**: Sebuah `class TranslationDataset` (mewarisi `Dataset` PyTorch) dibuat. Fungsi `__getitem__` di dalamnya bertugas mengambil pasangan kalimat, melakukan numerikalisasi (token-ke-indeks), dan menambahkan token `<SOS>` serta `<EOS>`.\n",
        "5.  **Collate Function (Padding)**: Sebuah `class MyCollate` kustom diimplementasikan. Fungsi ini sangat penting untuk mengambil sekelompok data (`batch`) dan menambahkan *padding* (`<PAD>`) agar semua sekuens dalam *batch* tersebut memiliki panjang yang seragam.\n",
        "6.  **DataLoader**: Data dibagi menjadi `train_loader` dan `val_loader` dengan `BATCH_SIZE=100`."
      ],
      "metadata": {
        "id": "pIxcB2mC1ZJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_c5U9zUlq7i",
        "outputId": "e48e57f6-1b07-4bc2-fe63-a422864e8c80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLmYCZ6vO-z8",
        "outputId": "2fe8f265-1534-46de-ea33-32308b4cc0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Berhasil memuat dan menggabungkan 2 file. Contoh data:\n",
            "                                             english  \\\n",
            "0  new jersey is sometimes quiet during autumn , ...   \n",
            "1  the united states is usually chilly during jul...   \n",
            "2  california is usually quiet during march , and...   \n",
            "3  the united states is sometimes mild during jun...   \n",
            "4  your least liked fruit is the grape , but my l...   \n",
            "\n",
            "                                              french  \n",
            "0  new jersey est parfois calme pendant l' automn...  \n",
            "1  les états-unis est généralement froid en juill...  \n",
            "2  california est généralement calme en mars , et...  \n",
            "3  les états-unis est parfois légère en juin , et...  \n",
            "4  votre moins aimé fruit est le raisin , mais mo...  \n",
            "Berhasil menggabungkan 2 file. Contoh data:\n",
            "                                             english  \\\n",
            "0  new jersey is sometimes quiet during autumn , ...   \n",
            "1  the united states is usually chilly during jul...   \n",
            "2  california is usually quiet during march , and...   \n",
            "3  the united states is sometimes mild during jun...   \n",
            "4  your least liked fruit is the grape , but my l...   \n",
            "\n",
            "                                              french  \n",
            "0  new jersey est parfois calme pendant l' automn...  \n",
            "1  les états-unis est généralement froid en juill...  \n",
            "2  california est généralement calme en mars , et...  \n",
            "3  les états-unis est parfois légère en juin , et...  \n",
            "4  votre moins aimé fruit est le raisin , mais mo...  \n",
            "Contoh data:\n",
            "                                             english  \\\n",
            "0  new jersey is sometimes quiet during autumn , ...   \n",
            "1  the united states is usually chilly during jul...   \n",
            "2  california is usually quiet during march , and...   \n",
            "3  the united states is sometimes mild during jun...   \n",
            "4  your least liked fruit is the grape , but my l...   \n",
            "\n",
            "                                              french  \n",
            "0  new jersey est parfois calme pendant l' automn...  \n",
            "1  les états-unis est généralement froid en juill...  \n",
            "2  california est généralement calme en mars , et...  \n",
            "3  les états-unis est parfois légère en juin , et...  \n",
            "4  votre moins aimé fruit est le raisin , mais mo...  \n",
            "Ukuran Kamus Inggris: 205\n",
            "Ukuran Kamus Prancis: 312\n",
            "DataLoader siap.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PERSIAPAN DATA (Text Preprocessing)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import io\n",
        "\n",
        "# Download & Load Dataset\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    with open('small_vocab_en.csv', 'r', encoding='utf-8') as f:\n",
        "        en_sentences = [line.strip() for line in f]\n",
        "\n",
        "    with open('small_vocab_fr.csv', 'r', encoding='utf-8') as f:\n",
        "        fr_sentences = [line.strip() for line in f]\n",
        "\n",
        "    if len(en_sentences) != len(fr_sentences):\n",
        "        print(f\"Error: Jumlah baris tidak sama! Inggris: {len(en_sentences)}, Prancis: {len(fr_sentences)}\")\n",
        "    else:\n",
        "        df = pd.DataFrame({\n",
        "            'english': en_sentences,\n",
        "            'french': fr_sentences\n",
        "        })\n",
        "\n",
        "        print(\"Berhasil memuat dan menggabungkan 2 file. Contoh data:\")\n",
        "        print(df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Pastikan 'small_vocab_en.csv' dan 'small_vocab_fr.csv' sudah di-upload.\")\n",
        "    df = pd.DataFrame({\"english\": [\"hello world\"], \"french\": [\"bonjour le monde\"]})\n",
        "\n",
        "print(\"Berhasil menggabungkan 2 file. Contoh data:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"Contoh data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Setup Tokenizer (Spacy)\n",
        "\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "spacy_fra = spacy.load('fr_core_news_sm')\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "def tokenize_fra(text):\n",
        "    return [tok.text.lower() for tok in spacy_fra.tokenizer(text)]\n",
        "\n",
        "# Setup Vocabulary (Kamus)\n",
        "class Vocabulary:\n",
        "    def __init__(self, tokenizer, freq_threshold=2):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.tokenizer = tokenizer\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        word_counts = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
        "\n",
        "# Membuat kamus\n",
        "freq_threshold = 2\n",
        "eng_vocab = Vocabulary(tokenize_eng, freq_threshold)\n",
        "fra_vocab = Vocabulary(tokenize_fra, freq_threshold)\n",
        "\n",
        "df_sample = df.sample(20000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "eng_vocab.build_vocabulary(df_sample['english'].tolist())\n",
        "fra_vocab.build_vocabulary(df_sample['french'].tolist())\n",
        "\n",
        "print(f\"Ukuran Kamus Inggris: {len(eng_vocab)}\")\n",
        "print(f\"Ukuran Kamus Prancis: {len(fra_vocab)}\")\n",
        "\n",
        "# Custom Dataset\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df, eng_vocab, fra_vocab):\n",
        "        self.df = df\n",
        "        self.eng_vocab = eng_vocab\n",
        "        self.fra_vocab = fra_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        eng_sentence = self.df.iloc[index]['english']\n",
        "        fra_sentence = self.df.iloc[index]['french']\n",
        "\n",
        "        eng_tokenized = [self.eng_vocab.stoi[\"<SOS>\"]] + self.eng_vocab.numericalize(eng_sentence) + [self.eng_vocab.stoi[\"<EOS>\"]]\n",
        "        fra_tokenized = [self.fra_vocab.stoi[\"<SOS>\"]] + self.fra_vocab.numericalize(fra_sentence) + [self.fra_vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        return torch.tensor(eng_tokenized), torch.tensor(fra_tokenized)\n",
        "\n",
        "# Collate Function (Padding)\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        source_seqs = [item[0] for item in batch]\n",
        "        target_seqs = [item[1] for item in batch]\n",
        "\n",
        "        # Pad sequences\n",
        "        source_padded = nn.utils.rnn.pad_sequence(source_seqs, batch_first=True, padding_value=self.pad_idx)\n",
        "        target_padded = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=self.pad_idx)\n",
        "\n",
        "        return source_padded, target_padded\n",
        "\n",
        "# Setup DataLoader\n",
        "train_df = df_sample.iloc[:19000]\n",
        "val_df = df_sample.iloc[19000:]\n",
        "\n",
        "train_dataset = TranslationDataset(train_df, eng_vocab, fra_vocab)\n",
        "val_dataset = TranslationDataset(val_df, eng_vocab, fra_vocab)\n",
        "\n",
        "PAD_IDX = eng_vocab.stoi[\"<PAD>\"]\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "collate_fn = MyCollate(pad_idx=PAD_IDX)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"DataLoader siap.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Definisi Arsitektur Transformer\n",
        "\n",
        "Arsitektur Transformer diimplementasikan \"from scratch\" dengan mendefinisikan setiap kelas komponen pembentuknya.\n",
        "\n",
        "**Kelas-kelas yang Didefinisikan:**\n",
        "1.  **`PositionalEncoding`**: Menambahkan informasi posisi ke *embedding* menggunakan formula `sin` dan `cos`, karena Transformer tidak memiliki pemahaman urutan secara abstrak.\n",
        "2.  **`MultiHeadAttention`**: Implementasi dari mekanisme *Scaled Dot-Product Attention*. Lapisan ini memproyeksikan Query, Key, dan Value, membaginya menjadi beberapa *head*, menghitung skor *attention*, dan menerapkan *mask*.\n",
        "3.  **`PositionwiseFeedForward`**: Jaringan MLP sederhana (Linear -> ReLU -> Linear) yang diterapkan setelah blok *attention*.\n",
        "4.  **`EncoderLayer`**: Satu blok Encoder yang terdiri dari `MultiHeadAttention` (self-attention) dan `PositionwiseFeedForward`, lengkap dengan *residual connection* dan *layer normalization*.\n",
        "5.  **`DecoderLayer`**: Satu blok Decoder yang terdiri dari dua `MultiHeadAttention` (self-attention dengan *look-ahead mask* dan *encoder-decoder attention*) serta `PositionwiseFeedForward`.\n",
        "6.  **`Seq2SeqTransformer`**: Kelas utama yang menggabungkan semua komponen. Kelas ini juga bertanggung jawab untuk:\n",
        "    * Meng-inisialisasi `nn.Embedding` untuk sumber dan target.\n",
        "    * Membuat tumpukan `EncoderLayer` dan `DecoderLayer`.\n",
        "    * Mendefinisikan fungsi `make_src_mask` (untuk padding) dan `make_trg_mask` (untuk padding + *look-ahead*).\n",
        "    * Mendefinisikan alur `forward` pass dari `src` dan `trg` hingga menghasilkan output *logits* (nilai keluaran mentah yang belum dinormalisasi)."
      ],
      "metadata": {
        "id": "fC8mO6KW1hKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# DEFINISI ARSITEKTUR TRANSFORMER\n",
        "# ==============================================================================\n",
        "import math\n",
        "\n",
        "# Class Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model).to(device)\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)].detach()\n",
        "\n",
        "# Class Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.q_linear(query)\n",
        "        K = self.k_linear(key)\n",
        "        V = self.v_linear(value)\n",
        "\n",
        "        # Split into heads\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attention, V)\n",
        "\n",
        "        # Concatenate heads\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.out_linear(context)\n",
        "\n",
        "# Class Position-wise Feed-Forward\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_dim, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "\n",
        "# Class Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, ff_dim, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.attention(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "# Class Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.encoder_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, ff_dim, dropout)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, trg_mask):\n",
        "        self_attn_output = self.self_attention(x, x, x, trg_mask)\n",
        "        x = x + self.dropout(self_attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        enc_attn_output = self.encoder_attention(x, enc_output, enc_output, src_mask)\n",
        "        x = x + self.dropout(enc_attn_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.norm3(x)\n",
        "        return x\n",
        "\n",
        "# Class Transformer Lengkap\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, num_heads, num_layers,\n",
        "                 ff_dim, max_len, dropout, device):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.device = device\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len, device)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != PAD_IDX).unsqueeze(1).unsqueeze(3)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        src_emb = self.dropout(self.pos_encoding(self.src_embedding(src)))\n",
        "        trg_emb = self.dropout(self.pos_encoding(self.trg_embedding(trg)))\n",
        "\n",
        "        enc_output = src_emb\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = trg_emb\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_output = layer(dec_output, enc_output, src_mask, trg_mask)\n",
        "\n",
        "        return self.fc_out(dec_output)\n",
        "\n",
        "print(\"Class-class arsitektur Transformer berhasil didefinisikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFVXftFvPGya",
        "outputId": "5306efde-5f32-4455-9558-0aed140b6ca4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class-class arsitektur Transformer berhasil didefinisikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Proses Pelatihan\n",
        "\n",
        "Tahap ini berfokus pada pelatihan model selama 1 epoch, dengan menampilkan metrik performa secara detail sesuai permintaan tugas.\n",
        "\n",
        "**Proses yang Dilakukan:**\n",
        "1.  **Inisialisasi**: Model `Seq2SeqTransformer` diinisialisasi dengan hyperparameter yang dipilih (seperti `D_MODEL=256`, `NUM_LAYERS=3` untuk mempercepat training). Bobot diinisialisasi menggunakan `xavier_uniform_` untuk stabilitas.\n",
        "2.  **Loss & Optimizer**: Kami menggunakan `CrossEntropyLoss` dengan `ignore_index=PAD_IDX` agar *padding* tidak berkontribusi pada *loss*. Optimizer yang digunakan adalah `Adam`.\n",
        "3.  **Fungsi `get_accuracy`**: Dibuat fungsi helper untuk menghitung akurasi per token, dengan mengabaikan token `<PAD>`.\n",
        "4.  **Fungsi `train_epoch` dan `evaluate`**:\n",
        "    * Kedua fungsi ini mengiterasi `DataLoader`.\n",
        "    * Melakukan *forward pass* dengan *slicing* token target yang tepat (`trg[:, :-1]` sebagai input dan `trg[:, 1:]` sebagai target) untuk *teacher forcing*.\n",
        "    * Menghitung *loss* dan *accuracy* untuk setiap *batch*.\n",
        "    * **Pelaporan per Batch**: Sesuai permintaan tugas, **TrainLoss**, **ValLoss**, dan **ValAcc** (atau TrainAcc) dicetak ke konsol **pada setiap akhir batch**.\n",
        "5.  **Eksekusi Training**: Model dilatih hanya untuk **1 epoch**."
      ],
      "metadata": {
        "id": "YvpbUAPM1mRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PROSES PELATIHAN\n",
        "# ==============================================================================\n",
        "\n",
        "# Inisialisasi Model dan Hyperparameter\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SRC_VOCAB_SIZE = len(eng_vocab)\n",
        "TRG_VOCAB_SIZE = len(fra_vocab)\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 3\n",
        "FF_DIM = 512\n",
        "MAX_LEN = 100\n",
        "DROPOUT = 0.1\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "model = Seq2SeqTransformer(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, D_MODEL, NUM_HEADS,\n",
        "                           NUM_LAYERS, FF_DIM, MAX_LEN, DROPOUT, DEVICE).to(DEVICE)\n",
        "\n",
        "# Inisialisasi bobot\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Fungsi Helper (Akurasi & Loop)\n",
        "def get_accuracy(output, target, pad_idx):\n",
        "    output = output.argmax(dim=-1)\n",
        "\n",
        "    non_pad_mask = (target != pad_idx)\n",
        "\n",
        "    correct = (output == target)[non_pad_mask].float()\n",
        "\n",
        "    if len(correct) == 0:\n",
        "        return torch.tensor(0.0)\n",
        "\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, pad_idx, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for i, batch in enumerate(loader):\n",
        "        src, trg = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg[:, :-1])\n",
        "\n",
        "        output_flat = output.reshape(-1, output.shape[2])\n",
        "        target_flat = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output_flat, target_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Hitung metrik\n",
        "        batch_loss = loss.item()\n",
        "        batch_acc = get_accuracy(output, trg[:, 1:], pad_idx).item()\n",
        "\n",
        "        epoch_loss += batch_loss\n",
        "        epoch_acc += batch_acc\n",
        "\n",
        "        # Laporan per Batch\n",
        "        print(f\"  Batch {i+1}/{len(loader)} | TrainLoss: {batch_loss:.4f} | TrainAcc: {batch_acc:.4f}\")\n",
        "\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion, pad_idx, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(loader):\n",
        "            src, trg = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg[:, :-1])\n",
        "            output_flat = output.reshape(-1, output.shape[2])\n",
        "            target_flat = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output_flat, target_flat)\n",
        "\n",
        "            # Hitung metrik\n",
        "            batch_loss = loss.item()\n",
        "            batch_acc = get_accuracy(output, trg[:, 1:], pad_idx).item()\n",
        "\n",
        "            epoch_loss += batch_loss\n",
        "            epoch_acc += batch_acc\n",
        "\n",
        "            # Laporan per Batch\n",
        "            print(f\"  Batch {i+1}/{len(loader)} | ValLoss: {batch_loss:.4f} | ValAcc: {batch_acc:.4f}\")\n",
        "\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "# Training\n",
        "print(f\"Memulai Training untuk {NUM_EPOCHS} Epoch...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    print(\"Menjalankan Training...\")\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, PAD_IDX, DEVICE)\n",
        "\n",
        "    print(\"\\nMenjalankan Validasi...\")\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, PAD_IDX, DEVICE)\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"HASIL EPOCH {epoch+1}:\")\n",
        "    print(f\"  Avg Train Loss: {train_loss:.4f} | Avg Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Avg Val Loss  : {val_loss:.4f} | Avg Val Acc  : {val_acc:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"Pelatihan selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5-hkgbCPLoD",
        "outputId": "c55e2842-2fed-489f-b4ab-a6f26077bae3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai Training untuk 1 Epoch...\n",
            "\n",
            "--- Epoch 1/1 ---\n",
            "Menjalankan Training...\n",
            "  Batch 1/190 | TrainLoss: 5.9446 | TrainAcc: 0.0075\n",
            "  Batch 2/190 | TrainLoss: 5.4090 | TrainAcc: 0.0838\n",
            "  Batch 3/190 | TrainLoss: 5.1529 | TrainAcc: 0.0923\n",
            "  Batch 4/190 | TrainLoss: 4.9883 | TrainAcc: 0.0908\n",
            "  Batch 5/190 | TrainLoss: 4.8319 | TrainAcc: 0.0807\n",
            "  Batch 6/190 | TrainLoss: 4.7717 | TrainAcc: 0.0787\n",
            "  Batch 7/190 | TrainLoss: 4.6265 | TrainAcc: 0.0739\n",
            "  Batch 8/190 | TrainLoss: 4.6473 | TrainAcc: 0.0787\n",
            "  Batch 9/190 | TrainLoss: 4.5519 | TrainAcc: 0.0790\n",
            "  Batch 10/190 | TrainLoss: 4.5403 | TrainAcc: 0.0946\n",
            "  Batch 11/190 | TrainLoss: 4.5334 | TrainAcc: 0.0932\n",
            "  Batch 12/190 | TrainLoss: 4.5081 | TrainAcc: 0.0977\n",
            "  Batch 13/190 | TrainLoss: 4.4688 | TrainAcc: 0.1000\n",
            "  Batch 14/190 | TrainLoss: 4.4991 | TrainAcc: 0.0824\n",
            "  Batch 15/190 | TrainLoss: 4.3906 | TrainAcc: 0.1074\n",
            "  Batch 16/190 | TrainLoss: 4.4095 | TrainAcc: 0.0883\n",
            "  Batch 17/190 | TrainLoss: 4.3741 | TrainAcc: 0.1018\n",
            "  Batch 18/190 | TrainLoss: 4.3972 | TrainAcc: 0.0907\n",
            "  Batch 19/190 | TrainLoss: 4.4309 | TrainAcc: 0.0998\n",
            "  Batch 20/190 | TrainLoss: 4.3879 | TrainAcc: 0.1008\n",
            "  Batch 21/190 | TrainLoss: 4.4100 | TrainAcc: 0.0972\n",
            "  Batch 22/190 | TrainLoss: 4.2990 | TrainAcc: 0.1129\n",
            "  Batch 23/190 | TrainLoss: 4.3588 | TrainAcc: 0.1014\n",
            "  Batch 24/190 | TrainLoss: 4.3411 | TrainAcc: 0.1076\n",
            "  Batch 25/190 | TrainLoss: 4.2887 | TrainAcc: 0.1062\n",
            "  Batch 26/190 | TrainLoss: 4.3099 | TrainAcc: 0.1148\n",
            "  Batch 27/190 | TrainLoss: 4.3076 | TrainAcc: 0.1046\n",
            "  Batch 28/190 | TrainLoss: 4.2488 | TrainAcc: 0.1279\n",
            "  Batch 29/190 | TrainLoss: 4.3142 | TrainAcc: 0.1271\n",
            "  Batch 30/190 | TrainLoss: 4.1790 | TrainAcc: 0.1355\n",
            "  Batch 31/190 | TrainLoss: 4.2138 | TrainAcc: 0.1205\n",
            "  Batch 32/190 | TrainLoss: 4.2591 | TrainAcc: 0.1255\n",
            "  Batch 33/190 | TrainLoss: 4.1406 | TrainAcc: 0.1365\n",
            "  Batch 34/190 | TrainLoss: 4.1873 | TrainAcc: 0.1412\n",
            "  Batch 35/190 | TrainLoss: 4.1625 | TrainAcc: 0.1413\n",
            "  Batch 36/190 | TrainLoss: 4.0865 | TrainAcc: 0.1450\n",
            "  Batch 37/190 | TrainLoss: 4.0855 | TrainAcc: 0.1453\n",
            "  Batch 38/190 | TrainLoss: 4.0864 | TrainAcc: 0.1549\n",
            "  Batch 39/190 | TrainLoss: 4.0602 | TrainAcc: 0.1625\n",
            "  Batch 40/190 | TrainLoss: 4.1091 | TrainAcc: 0.1597\n",
            "  Batch 41/190 | TrainLoss: 4.0114 | TrainAcc: 0.1659\n",
            "  Batch 42/190 | TrainLoss: 4.0296 | TrainAcc: 0.1700\n",
            "  Batch 43/190 | TrainLoss: 3.9719 | TrainAcc: 0.1835\n",
            "  Batch 44/190 | TrainLoss: 4.0505 | TrainAcc: 0.1697\n",
            "  Batch 45/190 | TrainLoss: 3.9406 | TrainAcc: 0.1830\n",
            "  Batch 46/190 | TrainLoss: 3.8771 | TrainAcc: 0.1939\n",
            "  Batch 47/190 | TrainLoss: 3.9768 | TrainAcc: 0.1802\n",
            "  Batch 48/190 | TrainLoss: 3.9506 | TrainAcc: 0.1915\n",
            "  Batch 49/190 | TrainLoss: 3.8327 | TrainAcc: 0.1901\n",
            "  Batch 50/190 | TrainLoss: 3.8481 | TrainAcc: 0.1906\n",
            "  Batch 51/190 | TrainLoss: 3.8417 | TrainAcc: 0.1944\n",
            "  Batch 52/190 | TrainLoss: 3.8393 | TrainAcc: 0.2015\n",
            "  Batch 53/190 | TrainLoss: 3.8367 | TrainAcc: 0.1977\n",
            "  Batch 54/190 | TrainLoss: 3.7285 | TrainAcc: 0.2026\n",
            "  Batch 55/190 | TrainLoss: 3.8303 | TrainAcc: 0.2038\n",
            "  Batch 56/190 | TrainLoss: 3.7858 | TrainAcc: 0.2085\n",
            "  Batch 57/190 | TrainLoss: 3.7650 | TrainAcc: 0.1901\n",
            "  Batch 58/190 | TrainLoss: 3.7529 | TrainAcc: 0.2024\n",
            "  Batch 59/190 | TrainLoss: 3.7142 | TrainAcc: 0.1883\n",
            "  Batch 60/190 | TrainLoss: 3.7692 | TrainAcc: 0.2051\n",
            "  Batch 61/190 | TrainLoss: 3.7362 | TrainAcc: 0.2041\n",
            "  Batch 62/190 | TrainLoss: 3.6839 | TrainAcc: 0.2107\n",
            "  Batch 63/190 | TrainLoss: 3.7050 | TrainAcc: 0.1945\n",
            "  Batch 64/190 | TrainLoss: 3.6782 | TrainAcc: 0.2178\n",
            "  Batch 65/190 | TrainLoss: 3.6139 | TrainAcc: 0.2230\n",
            "  Batch 66/190 | TrainLoss: 3.6767 | TrainAcc: 0.2151\n",
            "  Batch 67/190 | TrainLoss: 3.6582 | TrainAcc: 0.2130\n",
            "  Batch 68/190 | TrainLoss: 3.6492 | TrainAcc: 0.2088\n",
            "  Batch 69/190 | TrainLoss: 3.6534 | TrainAcc: 0.2044\n",
            "  Batch 70/190 | TrainLoss: 3.6430 | TrainAcc: 0.2216\n",
            "  Batch 71/190 | TrainLoss: 3.6193 | TrainAcc: 0.2129\n",
            "  Batch 72/190 | TrainLoss: 3.6354 | TrainAcc: 0.2113\n",
            "  Batch 73/190 | TrainLoss: 3.5494 | TrainAcc: 0.2232\n",
            "  Batch 74/190 | TrainLoss: 3.5678 | TrainAcc: 0.2166\n",
            "  Batch 75/190 | TrainLoss: 3.5699 | TrainAcc: 0.2164\n",
            "  Batch 76/190 | TrainLoss: 3.5357 | TrainAcc: 0.2302\n",
            "  Batch 77/190 | TrainLoss: 3.4984 | TrainAcc: 0.2344\n",
            "  Batch 78/190 | TrainLoss: 3.5829 | TrainAcc: 0.2128\n",
            "  Batch 79/190 | TrainLoss: 3.4698 | TrainAcc: 0.2307\n",
            "  Batch 80/190 | TrainLoss: 3.4891 | TrainAcc: 0.2276\n",
            "  Batch 81/190 | TrainLoss: 3.6212 | TrainAcc: 0.2182\n",
            "  Batch 82/190 | TrainLoss: 3.4793 | TrainAcc: 0.2330\n",
            "  Batch 83/190 | TrainLoss: 3.4202 | TrainAcc: 0.2419\n",
            "  Batch 84/190 | TrainLoss: 3.4655 | TrainAcc: 0.2467\n",
            "  Batch 85/190 | TrainLoss: 3.4453 | TrainAcc: 0.2494\n",
            "  Batch 86/190 | TrainLoss: 3.5140 | TrainAcc: 0.2269\n",
            "  Batch 87/190 | TrainLoss: 3.3758 | TrainAcc: 0.2485\n",
            "  Batch 88/190 | TrainLoss: 3.3901 | TrainAcc: 0.2402\n",
            "  Batch 89/190 | TrainLoss: 3.5169 | TrainAcc: 0.2268\n",
            "  Batch 90/190 | TrainLoss: 3.3917 | TrainAcc: 0.2377\n",
            "  Batch 91/190 | TrainLoss: 3.4230 | TrainAcc: 0.2404\n",
            "  Batch 92/190 | TrainLoss: 3.4173 | TrainAcc: 0.2268\n",
            "  Batch 93/190 | TrainLoss: 3.5010 | TrainAcc: 0.2206\n",
            "  Batch 94/190 | TrainLoss: 3.3932 | TrainAcc: 0.2397\n",
            "  Batch 95/190 | TrainLoss: 3.3783 | TrainAcc: 0.2413\n",
            "  Batch 96/190 | TrainLoss: 3.4125 | TrainAcc: 0.2586\n",
            "  Batch 97/190 | TrainLoss: 3.4327 | TrainAcc: 0.2464\n",
            "  Batch 98/190 | TrainLoss: 3.3332 | TrainAcc: 0.2341\n",
            "  Batch 99/190 | TrainLoss: 3.3811 | TrainAcc: 0.2473\n",
            "  Batch 100/190 | TrainLoss: 3.4448 | TrainAcc: 0.2406\n",
            "  Batch 101/190 | TrainLoss: 3.3230 | TrainAcc: 0.2709\n",
            "  Batch 102/190 | TrainLoss: 3.2872 | TrainAcc: 0.2483\n",
            "  Batch 103/190 | TrainLoss: 3.3056 | TrainAcc: 0.2617\n",
            "  Batch 104/190 | TrainLoss: 3.3186 | TrainAcc: 0.2292\n",
            "  Batch 105/190 | TrainLoss: 3.2253 | TrainAcc: 0.2607\n",
            "  Batch 106/190 | TrainLoss: 3.3773 | TrainAcc: 0.2399\n",
            "  Batch 107/190 | TrainLoss: 3.3713 | TrainAcc: 0.2266\n",
            "  Batch 108/190 | TrainLoss: 3.3699 | TrainAcc: 0.2319\n",
            "  Batch 109/190 | TrainLoss: 3.2026 | TrainAcc: 0.2711\n",
            "  Batch 110/190 | TrainLoss: 3.3651 | TrainAcc: 0.2553\n",
            "  Batch 111/190 | TrainLoss: 3.2662 | TrainAcc: 0.2579\n",
            "  Batch 112/190 | TrainLoss: 3.2027 | TrainAcc: 0.2781\n",
            "  Batch 113/190 | TrainLoss: 3.2704 | TrainAcc: 0.2399\n",
            "  Batch 114/190 | TrainLoss: 3.3218 | TrainAcc: 0.2448\n",
            "  Batch 115/190 | TrainLoss: 3.2884 | TrainAcc: 0.2550\n",
            "  Batch 116/190 | TrainLoss: 3.2609 | TrainAcc: 0.2493\n",
            "  Batch 117/190 | TrainLoss: 3.2555 | TrainAcc: 0.2633\n",
            "  Batch 118/190 | TrainLoss: 3.1824 | TrainAcc: 0.2660\n",
            "  Batch 119/190 | TrainLoss: 3.1897 | TrainAcc: 0.2879\n",
            "  Batch 120/190 | TrainLoss: 3.1544 | TrainAcc: 0.2829\n",
            "  Batch 121/190 | TrainLoss: 3.1783 | TrainAcc: 0.2898\n",
            "  Batch 122/190 | TrainLoss: 3.2259 | TrainAcc: 0.2748\n",
            "  Batch 123/190 | TrainLoss: 3.0226 | TrainAcc: 0.2908\n",
            "  Batch 124/190 | TrainLoss: 3.2009 | TrainAcc: 0.2741\n",
            "  Batch 125/190 | TrainLoss: 3.1398 | TrainAcc: 0.2815\n",
            "  Batch 126/190 | TrainLoss: 3.1135 | TrainAcc: 0.2829\n",
            "  Batch 127/190 | TrainLoss: 3.0796 | TrainAcc: 0.2819\n",
            "  Batch 128/190 | TrainLoss: 3.1597 | TrainAcc: 0.2716\n",
            "  Batch 129/190 | TrainLoss: 3.0911 | TrainAcc: 0.2878\n",
            "  Batch 130/190 | TrainLoss: 3.1381 | TrainAcc: 0.2795\n",
            "  Batch 131/190 | TrainLoss: 3.0965 | TrainAcc: 0.2667\n",
            "  Batch 132/190 | TrainLoss: 3.0057 | TrainAcc: 0.2809\n",
            "  Batch 133/190 | TrainLoss: 3.0516 | TrainAcc: 0.3077\n",
            "  Batch 134/190 | TrainLoss: 3.0867 | TrainAcc: 0.2899\n",
            "  Batch 135/190 | TrainLoss: 3.0924 | TrainAcc: 0.2932\n",
            "  Batch 136/190 | TrainLoss: 3.0606 | TrainAcc: 0.2818\n",
            "  Batch 137/190 | TrainLoss: 2.9694 | TrainAcc: 0.3110\n",
            "  Batch 138/190 | TrainLoss: 3.0376 | TrainAcc: 0.2805\n",
            "  Batch 139/190 | TrainLoss: 2.9674 | TrainAcc: 0.2958\n",
            "  Batch 140/190 | TrainLoss: 3.0684 | TrainAcc: 0.2992\n",
            "  Batch 141/190 | TrainLoss: 2.9396 | TrainAcc: 0.3025\n",
            "  Batch 142/190 | TrainLoss: 3.0855 | TrainAcc: 0.2839\n",
            "  Batch 143/190 | TrainLoss: 3.0385 | TrainAcc: 0.2926\n",
            "  Batch 144/190 | TrainLoss: 2.9873 | TrainAcc: 0.2959\n",
            "  Batch 145/190 | TrainLoss: 2.9938 | TrainAcc: 0.2920\n",
            "  Batch 146/190 | TrainLoss: 3.0029 | TrainAcc: 0.3041\n",
            "  Batch 147/190 | TrainLoss: 2.8885 | TrainAcc: 0.3060\n",
            "  Batch 148/190 | TrainLoss: 2.9888 | TrainAcc: 0.3020\n",
            "  Batch 149/190 | TrainLoss: 2.9143 | TrainAcc: 0.3030\n",
            "  Batch 150/190 | TrainLoss: 3.0489 | TrainAcc: 0.2802\n",
            "  Batch 151/190 | TrainLoss: 2.8984 | TrainAcc: 0.3192\n",
            "  Batch 152/190 | TrainLoss: 2.9435 | TrainAcc: 0.3029\n",
            "  Batch 153/190 | TrainLoss: 2.8411 | TrainAcc: 0.3307\n",
            "  Batch 154/190 | TrainLoss: 2.8771 | TrainAcc: 0.3213\n",
            "  Batch 155/190 | TrainLoss: 2.9639 | TrainAcc: 0.2923\n",
            "  Batch 156/190 | TrainLoss: 2.8862 | TrainAcc: 0.3064\n",
            "  Batch 157/190 | TrainLoss: 2.8525 | TrainAcc: 0.3194\n",
            "  Batch 158/190 | TrainLoss: 2.7820 | TrainAcc: 0.3453\n",
            "  Batch 159/190 | TrainLoss: 2.8634 | TrainAcc: 0.3251\n",
            "  Batch 160/190 | TrainLoss: 2.8816 | TrainAcc: 0.3333\n",
            "  Batch 161/190 | TrainLoss: 2.7846 | TrainAcc: 0.3306\n",
            "  Batch 162/190 | TrainLoss: 2.8350 | TrainAcc: 0.3423\n",
            "  Batch 163/190 | TrainLoss: 2.8201 | TrainAcc: 0.3198\n",
            "  Batch 164/190 | TrainLoss: 2.8995 | TrainAcc: 0.3099\n",
            "  Batch 165/190 | TrainLoss: 2.7464 | TrainAcc: 0.3476\n",
            "  Batch 166/190 | TrainLoss: 2.7670 | TrainAcc: 0.3549\n",
            "  Batch 167/190 | TrainLoss: 2.7191 | TrainAcc: 0.3625\n",
            "  Batch 168/190 | TrainLoss: 2.7736 | TrainAcc: 0.3431\n",
            "  Batch 169/190 | TrainLoss: 2.7623 | TrainAcc: 0.3462\n",
            "  Batch 170/190 | TrainLoss: 2.8360 | TrainAcc: 0.3360\n",
            "  Batch 171/190 | TrainLoss: 2.7590 | TrainAcc: 0.3497\n",
            "  Batch 172/190 | TrainLoss: 2.7252 | TrainAcc: 0.3492\n",
            "  Batch 173/190 | TrainLoss: 2.7093 | TrainAcc: 0.3500\n",
            "  Batch 174/190 | TrainLoss: 2.7528 | TrainAcc: 0.3598\n",
            "  Batch 175/190 | TrainLoss: 2.6174 | TrainAcc: 0.3610\n",
            "  Batch 176/190 | TrainLoss: 2.7685 | TrainAcc: 0.3524\n",
            "  Batch 177/190 | TrainLoss: 2.7183 | TrainAcc: 0.3699\n",
            "  Batch 178/190 | TrainLoss: 2.6780 | TrainAcc: 0.3560\n",
            "  Batch 179/190 | TrainLoss: 2.6548 | TrainAcc: 0.3626\n",
            "  Batch 180/190 | TrainLoss: 2.6768 | TrainAcc: 0.3775\n",
            "  Batch 181/190 | TrainLoss: 2.7130 | TrainAcc: 0.3598\n",
            "  Batch 182/190 | TrainLoss: 2.6993 | TrainAcc: 0.3616\n",
            "  Batch 183/190 | TrainLoss: 2.6448 | TrainAcc: 0.3745\n",
            "  Batch 184/190 | TrainLoss: 2.6229 | TrainAcc: 0.3746\n",
            "  Batch 185/190 | TrainLoss: 2.6593 | TrainAcc: 0.3669\n",
            "  Batch 186/190 | TrainLoss: 2.6595 | TrainAcc: 0.3610\n",
            "  Batch 187/190 | TrainLoss: 2.6480 | TrainAcc: 0.3661\n",
            "  Batch 188/190 | TrainLoss: 2.5184 | TrainAcc: 0.3941\n",
            "  Batch 189/190 | TrainLoss: 2.5952 | TrainAcc: 0.3841\n",
            "  Batch 190/190 | TrainLoss: 2.6192 | TrainAcc: 0.3898\n",
            "\n",
            "Menjalankan Validasi...\n",
            "  Batch 1/10 | ValLoss: 2.3060 | ValAcc: 0.4437\n",
            "  Batch 2/10 | ValLoss: 2.2738 | ValAcc: 0.4502\n",
            "  Batch 3/10 | ValLoss: 2.2776 | ValAcc: 0.4520\n",
            "  Batch 4/10 | ValLoss: 2.2967 | ValAcc: 0.4490\n",
            "  Batch 5/10 | ValLoss: 2.3130 | ValAcc: 0.4504\n",
            "  Batch 6/10 | ValLoss: 2.3672 | ValAcc: 0.4401\n",
            "  Batch 7/10 | ValLoss: 2.3080 | ValAcc: 0.4403\n",
            "  Batch 8/10 | ValLoss: 2.2436 | ValAcc: 0.4592\n",
            "  Batch 9/10 | ValLoss: 2.3836 | ValAcc: 0.4484\n",
            "  Batch 10/10 | ValLoss: 2.3053 | ValAcc: 0.4421\n",
            "--------------------------------------------------\n",
            "HASIL EPOCH 1:\n",
            "  Avg Train Loss: 3.4947 | Avg Train Acc: 0.2364\n",
            "  Avg Val Loss  : 2.3075 | Avg Val Acc  : 0.4475\n",
            "--------------------------------------------------\n",
            "Pelatihan selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Proses Inferensi (Translation)\n",
        "\n",
        "Tahap ini mendokumentasikan proses penggunaan model yang telah dilatih untuk menerjemahkan kalimat baru dari bahasa Inggris ke bahasa Prancis.\n",
        "\n",
        "**Proses yang Dilakukan:**\n",
        "1.  **Fungsi `translate_sentence`**: Dibuat sebuah fungsi khusus untuk inferensi.\n",
        "2.  **Mode Autoregressive**: Tidak seperti saat training, proses ini berjalan *autoregressive* (kata demi kata):\n",
        "    * Kalimat sumber diproses oleh **Encoder** *hanya satu kali* untuk mendapatkan representasi konteks (`enc_output`).\n",
        "    * **Decoder** dimulai dengan token `<SOS>`.\n",
        "    * Decoder memprediksi kata berikutnya (`pred_token`).\n",
        "    * `pred_token` tersebut kemudian digabungkan ke input Decoder untuk memprediksi kata selanjutnya.\n",
        "    * Proses ini diulang hingga model memprediksi token `<EOS>` atau mencapai panjang maksimum.\n",
        "3.  **Uji Coba**: Kami menunjukkan penerapan fungsi ini pada kalimat dari *validation set* dan pada kalimat kustom baru.\n",
        "    * **Hasil**: Karena model hanya dilatih 1 epoch, hasil terjemahan yang dihasilkan masih acak. Fokusnya adalah untuk mendemonstrasikan bahwa *pipeline* inferensi berfungsi dengan benar."
      ],
      "metadata": {
        "id": "mKnDeU-w1r6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PROSES INFERENSI (TRANSLATION)\n",
        "# ==============================================================================\n",
        "\n",
        "def translate_sentence(model, sentence, eng_vocab, fra_vocab, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Membuat Token untuk kalimat sumber\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [tok.text.lower() for tok in spacy_eng.tokenizer(sentence)]\n",
        "    else:\n",
        "        tokens = [tok.lower() for tok in sentence]\n",
        "\n",
        "    # Transform ke angka + <SOS> dan <EOS>\n",
        "    tokens_numerical = [eng_vocab.stoi[\"<SOS>\"]] + [eng_vocab.stoi.get(token, eng_vocab.stoi[\"<UNK>\"]) for token in tokens] + [eng_vocab.stoi[\"<EOS>\"]]\n",
        "    src_tensor = torch.LongTensor(tokens_numerical).unsqueeze(0).to(device)\n",
        "\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.src_embedding(src_tensor)\n",
        "        enc_output = model.pos_encoding(enc_output)\n",
        "        for layer in model.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "    # Proses decoding (autoregressive)\n",
        "    trg_indices = [fra_vocab.stoi[\"<SOS>\"]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dec_output = model.trg_embedding(trg_tensor)\n",
        "            dec_output = model.pos_encoding(dec_output)\n",
        "            for layer in model.decoder_layers:\n",
        "                dec_output = layer(dec_output, enc_output, src_mask, trg_mask)\n",
        "\n",
        "            output = model.fc_out(dec_output)\n",
        "\n",
        "        pred_token = output.argmax(2)[:, -1].item()\n",
        "        trg_indices.append(pred_token)\n",
        "\n",
        "        if pred_token == fra_vocab.stoi[\"<EOS>\"]:\n",
        "            break\n",
        "\n",
        "    # Transform angka kembali jadi kata-kata\n",
        "    trg_tokens = [fra_vocab.itos[i] for i in trg_indices]\n",
        "\n",
        "    return trg_tokens[1:]\n",
        "\n",
        "print(\"\\n--- UJI COBA INFERENSI ---\")\n",
        "\n",
        "example_idx = 5\n",
        "src_text = val_df.iloc[example_idx]['english']\n",
        "trg_text = val_df.iloc[example_idx]['french']\n",
        "\n",
        "print(f\"Kalimat Sumber (Inggris): {src_text}\")\n",
        "print(f\"Terjemahan Seharusnya (Prancis): {trg_text}\")\n",
        "\n",
        "translation_result = translate_sentence(model, src_text, eng_vocab, fra_vocab, DEVICE)\n",
        "print(f\"Hasil Terjemahan Model: {' '.join(translation_result)}\")\n",
        "\n",
        "custom_sentence = \"a cat is sitting on the roof\"\n",
        "print(f\"\\nKalimat Custom: {custom_sentence}\")\n",
        "translation_result = translate_sentence(model, custom_sentence, eng_vocab, fra_vocab, DEVICE)\n",
        "print(f\"Hasil Terjemahan Model: {' '.join(translation_result)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTqdVq-wPSBp",
        "outputId": "014ab916-01ca-4766-caa6-d511c53cf765"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- UJI COBA INFERENSI ---\n",
            "Kalimat Sumber (Inggris): he likes grapefruit , grapes , and oranges .\n",
            "Terjemahan Seharusnya (Prancis): il aime le pamplemousse , les raisins et les oranges .\n",
            "Hasil Terjemahan Model: il aime les les , les les et les les . <EOS>\n",
            "\n",
            "Kalimat Custom: a cat is sitting on the roof\n",
            "Hasil Terjemahan Model: elle aime est le , . . <EOS>\n"
          ]
        }
      ]
    }
  ]
}